{"cells": [{"cell_type": "code", "execution_count": 13, "id": "d6610f5d-d587-4d0e-ab56-a04b9b8bf46f", "metadata": {"tags": []}, "outputs": [], "source": "# -------------------------\n# 1. Crear SparkSession\n# -------------------------\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, hour, count, collect_list, map_from_entries, struct\n\nspark = SparkSession.builder \\\n    .appName(\"MarketingCampaignAnalysis\") \\\n    .getOrCreate()\n\n# -------------------------\n# 2. Cargar datasets desde HDFS\n# -------------------------\nad_campaigns = spark.read.json(\"hdfs:///user/spark/ass1/ad_campaigns_data.json\")\nuser_profiles = spark.read.json(\"hdfs:///user/spark/ass1/user_profile_data.json\")\nstores = spark.read.json(\"hdfs:///user/spark/ass1/store_data.json\")\n\n# -------------------------\n# 3. Preprocesamiento com\u00fan\n# -------------------------\n# Extraer fecha y hora desde event_time\nad_campaigns = ad_campaigns.withColumn(\"date_\", to_date(col(\"event_time\"))) \\\n                           .withColumn(\"hour\", hour(col(\"event_time\")))\n"}, {"cell_type": "code", "execution_count": 14, "id": "03d485fd-0c68-4fd5-9ac8-a0b0cde4bfbf", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+----+-------+-----+----------+--------+\n|campaign_id|date_     |hour|os_type|click|impression|video_ad|\n+-----------+----------+----+-------+-----+----------+--------+\n|ABCDFAE    |2018-10-12|13  |android|1    |1         |1       |\n|ABCDFAE    |2018-10-12|13  |ios    |0    |1         |0       |\n+-----------+----------+----+-------+-----+----------+--------+\n\n"}], "source": "def query_q1(ad_campaigns):\n    # Contar eventos agrupados\n    grouped = ad_campaigns.groupBy(\"campaign_id\", \"date_\", \"hour\", \"os_type\", \"event_type\") \\\n                          .count()\n\n    # Reestructurar el resultado (pivot para cada tipo de evento)\n    pivoted = grouped.groupBy(\"campaign_id\", \"date_\", \"hour\", \"os_type\") \\\n                     .pivot(\"event_type\") \\\n                     .sum(\"count\") \\\n                     .fillna(0)\n\n    return pivoted\n\nq1_result = query_q1(ad_campaigns)\nq1_result.show(truncate=False)\n"}, {"cell_type": "code", "execution_count": 15, "id": "a6860dbf-5e34-4a62-91e3-47ebd03c2d41", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+----+-------------+-----+----------+--------+\n|campaign_id|date_     |hour|store_name   |click|impression|video_ad|\n+-----------+----------+----+-------------+-----+----------+--------+\n|ABCDFAE    |2018-10-12|13  |McDonald     |1    |2         |0       |\n|ABCDFAE    |2018-10-12|13  |shoppers stop|0    |0         |1       |\n|ABCDFAE    |2018-10-12|13  |BurgerKing   |1    |1         |0       |\n+-----------+----------+----+-------------+-----+----------+--------+\n\n"}], "source": "def query_q2(ad_campaigns, stores):\n    # Expandir place_ids en store_data\n    stores_exploded = stores.withColumn(\"place_id\", explode(col(\"place_ids\"))) \\\n                            .drop(\"place_ids\")\n\n    # Join entre campa\u00f1as y stores\n    joined = ad_campaigns.join(stores_exploded, \"place_id\", \"inner\")\n\n    grouped = joined.groupBy(\"campaign_id\", \"date_\", \"hour\", \"store_name\", \"event_type\") \\\n                    .count()\n\n    pivoted = grouped.groupBy(\"campaign_id\", \"date_\", \"hour\", \"store_name\") \\\n                     .pivot(\"event_type\") \\\n                     .sum(\"count\") \\\n                     .fillna(0)\n\n    return pivoted\n\nq2_result = query_q2(ad_campaigns, stores)\nq2_result.show(truncate=False)\n"}, {"cell_type": "code", "execution_count": 16, "id": "e1b96e62-2d6e-4fec-a3db-e81615de1c54", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+----+------+-----+----------+--------+\n|campaign_id|date_     |hour|gender|click|impression|video_ad|\n+-----------+----------+----+------+-----+----------+--------+\n|ABCDFAE    |2018-10-12|13  |male  |1    |1         |1       |\n|ABCDFAE    |2018-10-12|13  |female|0    |1         |0       |\n+-----------+----------+----+------+-----+----------+--------+\n\n"}], "source": "def query_q3(ad_campaigns, user_profiles):\n    # Join entre campa\u00f1as y perfiles de usuarios\n    joined = ad_campaigns.join(user_profiles, \"user_id\", \"inner\")\n\n    grouped = joined.groupBy(\"campaign_id\", \"date_\", \"hour\", \"gender\", \"event_type\") \\\n                    .count()\n\n    pivoted = grouped.groupBy(\"campaign_id\", \"date_\", \"hour\", \"gender\") \\\n                     .pivot(\"event_type\") \\\n                     .sum(\"count\") \\\n                     .fillna(0)\n\n    return pivoted\n\nq3_result = query_q3(ad_campaigns, user_profiles)\nq3_result.show(truncate=False)\n"}, {"cell_type": "code", "execution_count": 17, "id": "360b2f27-afd9-4bd1-a9b6-a79451b25408", "metadata": {"tags": []}, "outputs": [], "source": "q1_result.write.mode(\"overwrite\").json(\"hdfs:///user/spark/ass1/q1\")\nq2_result.write.mode(\"overwrite\").json(\"hdfs:///user/spark/ass1/q2\")\nq3_result.write.mode(\"overwrite\").json(\"hdfs:///user/spark/ass1/q3\")"}, {"cell_type": "code", "execution_count": null, "id": "899ab500-7a22-43dc-ac38-0a1b74b8c204", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}